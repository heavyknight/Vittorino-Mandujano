{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicación de la Regresión Logística para el Análisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptado por http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se entrenó una regresión logística para clasificar opiniones de un dataset de 50K IMDb  recolectado por Maas el. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "La base de datos consiste en 50,000 opiniones de películas del original \"entrenamiento\" y \"testeo\". Las etiquetas son binarias y contienen 25,000 comentarios positivos y 25,000 comentarios negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos la librería Panda\n",
    "import pandas as pd\n",
    "\n",
    "# Y leemos el archivo desde la hoja de cálculo.\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "\n",
    "#revisamos el contenido de los 5 últimos elementos con tail()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería Numpy\n",
    "import numpy as np\n",
    "#reordenamos el contenido de la hoja de cálculo\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de la Información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizó la función 'tokenizer' para convertir el texto en una matriz de palabras así como la conversión a minúsculas y el filtrado de símbolos que no tienen trascendencia ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería nltk para tratar con cadenas de texto\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función 'stopwords.words('english')'extraimos palabras importantes y los almacenamos en matrices con los nombres 'stop', 'negativos' y 'pronom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=[u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', \n",
    "      u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing',\n",
    "      u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by',\n",
    "      u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above',\n",
    "      u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further',\n",
    "      u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', \n",
    "      u'more', u'most', u'other', u'some', u'such',  u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', \n",
    "      u't', u'can', u'will', u'just', u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativos=[ u'no', u'nor', u'not', u\"don't\", u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn',\n",
    "      u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'mightn',\n",
    "      u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn',\n",
    "      u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronom=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\",\n",
    "        u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\",\n",
    "        u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs',\n",
    "        u'themselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código del 'tokenizer'\n",
    "porter = PorterStemmer()\n",
    "exclamation = '!'\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('!', ' ! ', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+' + exclamation + ']', ' ', text) + ' '.join(emoticons).replace('-', '')\n",
    "    text=text.split()\n",
    "    #text = [w for w in text.split() if w not in stop]\n",
    "    #tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos una prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', ':)', 'is', 'a', 'test', '!', ':-):)', ':)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se crea un buffer para trabajar con el texto de la hoja de cálculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 1: Definir nuevas características accorde con https://web.stanford.edu/~jurafsky/slp3/5.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de las características del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las funciones que extraen las características del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función cuenta la cantidad de palabras positivas de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"positive-words.txt\") as word_file:\n",
    "    positive_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "def is_positive_word(word):\n",
    "    return word.lower() in positive_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función cuenta la cantidad de palabras negativas de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"negative-words.txt\") as word_file:\n",
    "    negative_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "def is_negative_word(word):\n",
    "    return word.lower() in negative_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función cuenta la cantidad de palabras negativas como : no, not, nor, can't, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_neg_word(word):\n",
    "    if(unicode(word.lower(), \"utf-8\") in negativos):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función cuenta la cantidad de pronombres de una texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_first_second_p(word):\n",
    "    if(unicode(word.lower(), \"utf-8\") in pronom):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función cuenta la cantidad de signos de interrogación dentro del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sign_word(word):\n",
    "    return word==\"!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función tiene como entrada un texto y lo convierte en una matriz [ x0, x1, x2, x3, x4, x5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformFeatures(unTexto): #ingresar buffer\n",
    "    x=np.array([0,0,0,0,0,0])\n",
    "    texto=np.array(unTexto)\n",
    "    for i in range(texto.size):\n",
    "        if(is_positive_word(texto[i])):\n",
    "            x[0]+=1\n",
    "        if(is_negative_word(texto[i])):\n",
    "            x[1]+=1\n",
    "        if(is_neg_word(texto[i])):\n",
    "            x[2]+=1\n",
    "        if(is_first_second_p(texto[i])):\n",
    "            x[3]+=1\n",
    "        if(is_sign_word(texto[i])):\n",
    "            x[4]+=1\n",
    "    x[5]=np.log(texto.size)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta función se extrae todo los textos de la hoja de calculo y los almacena en un array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text  = next(doc_stream)\n",
    "        texto = tokenizer(text[0])\n",
    "        label = int(text[1])\n",
    "        docs.append(transformFeatures(texto))\n",
    "        y.append(label)\n",
    "    return np.array(docs), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos los textos de la hoja de cálculo y realizamos la conversión a una matriz con, en este caso, las 6 variables y características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flujo=stream_docs(path='shuffled_movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#esta parte del código toma aproximadamente 1 o 2 min\n",
    "Xx, Yy=get_batch(flujo, 50000)\n",
    "Xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 2: Implementación de la Regresión Logística usando Regularización acorde con https://web.stanford.edu/~jurafsky/slp3/5.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística con Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para trabajar con la regresion logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresionLog(Xtraining,Ytraining,inicio,final,iteraciones, alpha):\n",
    "    #Donde:\n",
    "    # Xtraining: Elementos de entrada de entrenamiento\n",
    "    # Ytraining: Elementos de salida de entrenamiento\n",
    "    # inicio   : index del primer elemento en Xtraining y Ytraining\n",
    "    # final    : index del elemento después del último de Xtraining y Ytraining\n",
    "    # iteraciones : Cantidad de repeticiones del proceso.\n",
    "    # alpha    : El número que multiplica al gradiente.\n",
    "    \n",
    "    Ww=10*np.array(np.random.rand(6))-5 #Los pesos inicializados con valores aleatorios\n",
    "    b=0 #El bias\n",
    "    beta=0.999 #importante variable en la Regresión Logística con Regularización\n",
    "    for j in range(iteraciones):\n",
    "        for i in range(inicio,final):\n",
    "            Cc=1.0/(1+np.exp(-np.matmul(Xtraining[i],Ww)-b)) - Ytraining[i]\n",
    "            b=beta*b-alpha*Cc\n",
    "            Ww=beta*Ww-alpha*Cc*Xtraining[i] #Esta es la expresión iterativa de de la Regresión Logística con Regularización\n",
    "    return Ww,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo: Realizamos una iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04898685, -0.04621436, -0.0156574 ,  0.00149933, -0.00394275,\n",
      "       -0.00234278]), -0.0007441865295391508)\n"
     ]
    }
   ],
   "source": [
    "Ww,b=regresionLog(Xx,Yy,0,45000,50,0.0001)\n",
    "print(Ww,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y luego otra para corroborar su convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04898685, -0.04621436, -0.0156574 ,  0.00149933, -0.00394275,\n",
      "       -0.00234278]), -0.0007441865295391508)\n"
     ]
    }
   ],
   "source": [
    "Ww,b=regresionLog(Xx,Yy,0,45000,100,0.0001)\n",
    "print(Ww,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de $\\alpha$ fue escogido de forma empirica, en base a ensayo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos funciones que nos permitan medir la precisión del proceso, primero creamos la función SIGMOIDE\n",
    "$$ F(z)=\\frac{1}{1+\\exp(-z)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcionResult(W,b,X,inicio,fin):\n",
    "    Y=np.round(1.0/(1+np.exp(-np.matmul(X[inicio:fin],Ww)-b)))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función se encarga de calcular el porcentaje de acierto del porceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculoAcc(Ytest,Ypred,size):\n",
    "    acierto=0\n",
    "    for i in range(size):\n",
    "        if(Ytest[i]==Ypred[i]):\n",
    "            acierto+=1\n",
    "    return (1.0*acierto)/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=funcionResult(Ww,b,Xx,0,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7178"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculoAcc(Y_test,Yy,50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con una tasa de acierto del 71,78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizo el entrenamiento cruzado donde se formaron 10 grupos de 5000 textos y después se corroboraron con todo el universo de pruebas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test numero', 1)\n",
      "(array([ 7.09826827e-03, -5.35924595e-03, -1.97437050e-03,  2.10011696e-03,\n",
      "       -5.69098750e-05, -2.25075978e-04]), -5.180512391145822e-05)\n",
      "0.60354\n",
      "('Test numero', 2)\n",
      "(array([ 6.94571692e-03, -5.82104865e-03, -1.82751308e-03,  1.28016559e-03,\n",
      "       -7.61435821e-04, -4.82729307e-05]), -1.1554800416377122e-05)\n",
      "0.65808\n",
      "('Test numero', 3)\n",
      "(array([ 0.00678149, -0.00685713, -0.00232508,  0.00115488, -0.00016672,\n",
      "       -0.00024902]), -6.516498576348736e-05)\n",
      "0.69678\n",
      "('Test numero', 4)\n",
      "(array([ 0.00667454, -0.00623989, -0.00234839, -0.00116093, -0.00034825,\n",
      "       -0.00021956]), -3.328029209493929e-05)\n",
      "0.70122\n",
      "('Test numero', 5)\n",
      "(array([ 5.65463852e-03, -6.91844043e-03, -2.64180712e-03, -2.04342983e-03,\n",
      "       -6.22819745e-06, -6.13961814e-04]), -0.00010293213477328954)\n",
      "0.6194\n",
      "('Test numero', 6)\n",
      "(array([ 7.15345486e-03, -5.23805676e-03, -1.88995372e-03,  2.00590800e-03,\n",
      "       -2.98022070e-04,  5.52617048e-05]), 2.0549625039643275e-05)\n",
      "0.5969\n",
      "('Test numero', 7)\n",
      "(array([ 0.0075667 , -0.00585951, -0.00180953,  0.0022865 , -0.00026444,\n",
      "        0.00048701]), 0.00010296030877682619)\n",
      "0.58998\n",
      "('Test numero', 8)\n",
      "(array([ 0.00720449, -0.0053597 , -0.00173622,  0.00254677,  0.00041111,\n",
      "        0.00016455]), 3.0683070695838334e-05)\n",
      "0.5729\n",
      "('Test numero', 9)\n",
      "(array([ 0.00636925, -0.00668156, -0.00197957, -0.00053378, -0.00045964,\n",
      "       -0.00029726]), -8.081830650110261e-05)\n",
      "0.71314\n",
      "('Test numero', 10)\n",
      "(array([ 0.00442519, -0.00724262, -0.00254937, -0.00183533, -0.00056476,\n",
      "       -0.0005641 ]), -0.00010027424774860617)\n",
      "0.58082\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Test numero\",i+1)\n",
    "    Ww,b=regresionLog(Xx,Yy,5000*i,5000*i+5000,500,0.00001)\n",
    "    print(Ww,b)\n",
    "    Y_test=funcionResult(Ww,b,Xx,0,50000)\n",
    "    print(calculoAcc(Y_test,Yy,50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vemos que la mejor prueba es aquella que tiene un acierto de un 71,9% correspondiente a los pesos:\n",
    "$\\theta =$ [ 0.00636925, -0.00668156, -0.00197957, -0.00053378, -0.00045964,\n",
    "       -0.00029726]\n",
    "y un $bias=$-8.081830650110261e-05\n",
    "\n",
    "Observación: Los valores pueden variar por cada entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
